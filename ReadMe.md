# Overview 
This is the documentation for \<placeholder for paper name\>. 


# Preliminaries
You will need to download the datasets used in this work.

# Resources
@Kiet can you write what architecture you used for running the code? 

# Libraries 
Please see ```requirements.txt``` for a list of all the packages used for obtaining our empirical results. 

# Datasets 

### MovieLens 
You can access the dataset at \< placeholder for url\>. 
Please make sure it follows the file structure: 
```
ml-1m/
├── movies.dat     # Movie metadata (title, genres)
├── ratings.dat    # User–movie ratings
├── users.dat      # User demographic information
└── README         # Dataset description and usage notes
```

# Reproducing Paper Results 
The process for running our empirical results follows several steps. We will show explicit flow for MovieLens but all datasets will follow same formatting. 

### Step 1: Formatting unordered ratings as full ordering of preferences 
In order to convert the ratings in the dataset into a fully ordered set of preferences we use an SVD-based recommender system. To run this please use ```python3 pipline.py```

This should produce a file called ```recommendations.csv```. This will follow the format: 
```
          User_ID  Movie_ID  Estimated_Rating
0            5412       260            5.0999
1            5412      1198            5.0715
2            5412       922            5.0708
3            5412        50            5.0672
4            5412      3114            5.0464
...           ...       ...               ...
21396828     3303      1495            2.3406
21396829     3303      1739            2.3406
21396830     3303        66            2.2522
21396831     3303      1323            2.2484
21396832     3303       810            2.2118
```


### Step 2: Aggregating over the fully ordered preferences 
You will then need to generate *one* aggregated list over all the user-level preference lists generated by our recommendation system. To run this please use: ```python3 evaluate_aggregation.py --outdir <name of the output directory to store results>```. 

This will produce a folder containing the following structure: 
```
consensus_results/
├── BordaCount.txt   # Borda count aggregation
├── CG.txt           # Condorcet-based method
├── CombANZ.txt      # ANZ score fusion
├── CombMAX.txt      # MAX fusion
├── CombMIN.txt      # MIN fusion
├── CombMNZ.txt      # MNZ fusion
├── CombSUM.txt      # SUM fusion
├── DIBRA.txt        # Distance-based rank 
...
└── RRF.txt       

```
More specifically, each file is the output of a specific rank aggregation method, e.g. BordaCount, organized as follows: 
```
# Method: BordaCount
# Rank ItemID Score
1 2905 21146981.000000
2 668 20980313.000000
3 3338 20919789.000000
4 3470 20577506.000000
5 1236 20507166.000000
6 649 20448665.000000
...
```
Here, the ItemID signifies the index of each movie in MovieLens dataset (@Kiet: make sure this is correct please). 

Please note that we have reimplemented each of the rank aggregation methods based on the versions included in https://github.com/nercms-mmap/RankAggregation-Lib. We performed a validation to make sure that our results are aligned with theirs. The only excluded methods were: @Kiet please fill in. 

### Step 3: Evaluation 
We provide several directions of analysis in our empirical results. 

First, we evaluate the *utility* of each rank aggregation method with respect to the average Kendall Tau distance between each user-level ordered preference list and the final aggregated list. To run this, please use: ```python3 kendall_tau_calc.py```. 

Second, we evaluate the ability of each method to satisfy a series of proportionality axioms. To run this, please use: ``` TBD ```.

Third, we evaluate each method's performance on a series of canonical metrics used in recommendation systems to evaluate the visibility allocated to items of different popularity. To run this, please use: ``` TBD. ``` 